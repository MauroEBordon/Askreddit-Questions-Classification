{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Build Datasets\n","\n","I made the corpus here: [kaggle.com/mauroebordon/creating-a-qa-corpus-from-askreddit/](https://www.kaggle.com/mauroebordon/creating-a-qa-corpus-from-askreddit/)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import re\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","db_df = pd.read_csv(\"db/ask-reddit-corpus.csv\", index_col=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Extraction\n","\n","Only Tokens, Lemmas, PoS AND stopword filtering for now.\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","def extract_features(df: pd.DataFrame, toks=True, lems=True, pos=True):\n","\n","    \"\"\"\n","    idea: las collocations\n","    \"\"\"\n","    lmtzr = WordNetLemmatizer()\n","   \n","    #Filtramos las preguntas demasiado extensas\n","    db_df = df.copy()[df.Q.apply(lambda x: len(str(x)) <50)]\n","\n","    #remove stopwords\n","    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')        \n","    db_df[\"Qless\"] = db_df.Q.str.replace(pattern, '')\n","    db_df[\"Aless\"] = db_df.ANS.str.replace(pattern, '')\n","        \n","    #Obtenemos los tokens\n","    if toks:\n","        db_df[\"Qtoks\"] = [word_tokenize(w) for w in db_df[\"Qless\"]]\n","        db_df[\"Atoks\"] = [word_tokenize(w) for w in db_df[\"Aless\"]]\n","\n","    #Obtenemos los lemmas\n","    if lems:\n","        db_df[\"Qlemmas\"] = [' '.join(lmtzr.lemmatize(t) for t in qes) for qes in db_df[\"Qtoks\"]]\n","        db_df[\"Alemmas\"] = [' '.join(lmtzr.lemmatize(t) for t in qes) for qes in db_df[\"Atoks\"]]\n","\n","\n","    # Par Tok & POS\n","    if pos:\n","        db_df[\"Qpos\"] = [pos_tag(word_tokenize(w)) for w in db_df[\"Qless\"]]\n","        db_df[\"Apos\"] = [pos_tag(word_tokenize(w)) for w in db_df[\"Aless\"]]\n","\n","    \n","    \n","    db_df = db_df[[\"id\", \"Q\", \"Qscore\", \"Qless\", \"Qlemmas\", \"Qpos\", \"Qtoks\", \"ANS\", \"ANSscore\", \"Aless\", \"Alemmas\", \"Apos\", \"Atoks\"]]\n","\n","    #eliminamos los repetidos\n","    db_df = db_df.groupby(\"Q\", as_index=False).first()\n","\n","\n","    #solo consideramos con las preguntas con 2 o mÃ¡s upvotes (~32k)\n","    #sdf = df[df.Qscore > 1]\n","\n","    #filtramos las respuestas muy cortas, nos deja un total de 25k. \n","    #sdf = sdf.copy()[sdf.ANS.apply(lambda x: len(str(x)) > 15)]\n","\n","\n","    return db_df\n","    \n","\n","#usar pickle no sirve para comprimir naranja\n","df = extract_features(db_df)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_csv(\"db/features.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# Building the Test Dataset for Clustering Comparition\n","\n","Small manual clusters to get a little sence if clustering is working\n","\n","Esto quizas es muy muy tonto, pero solamente ordenar alfabeticamente las preguntas es una buena forma de encontrar similaridad.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"db/features.csv\", index_col=0)\n","\n","n_clus = 5 #no cambiar sin agregar otro\n","n_sam = 100\n","\n","df = df.groupby(\"Q\", as_index=False).first()\n","\n","#testeo de algunos \"tipos\" de preguntas precatios y de concepto\n","q_ids = []\n","q_ids += list(df[df.Q.str.contains(r'favorite movie')].id.sample(n_sam))\n","q_ids += list(df[df.Q.str.contains(r'ever seen')].id.sample(n_sam))\n","q_ids += list(df[df.Q.str.contains(r'advice')].id.sample(n_sam))\n","q_ids += list(df[df.Q.str.contains(r'history')].id.sample(n_sam))\n","q_ids += list(df[df.Q.str.contains(r'book')].id.sample(n_sam))\n","\n","# Mostramos un poquito como queda\n","# for j in range(n_clus):\n","#    print(f\"\\ncluster {j}\")\n","#    for i in range(n_sam):\n","#       print(f\"   {df.loc[q_ids[j*n_clus+i]].Q}\")\n","\n","\n","q_ids = [(x, int(i/n_sam)) for i, x in enumerate(q_ids)] \n","\n","total = n_clus*n_sam\n","\n","test_db = []\n","for i in range(total):\n","    for j in range(1, total-i):\n","        este = q_ids[i]\n","        otro = q_ids[j+i]\n","        test_db += [[este[0], otro[0], este[1] == otro[1]]]\n","\n","\n","test_db = pd.DataFrame(test_db)\n","test_db = test_db[test_db[0] != test_db[1]]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_db"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_db.to_csv(\"db/test_db.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
