{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "~~usar varios clusters, uno de 3grams, uno de nouns, uno de embeddings~~   \n",
    "~~con esto buscar un clasificador multilabel (o simplemente agregarlos como features del modelo que le fedee al clasificador)~~  \n",
    "\n",
    "Des métodos para clusterizar:    \n",
    "-primero hago cluster separados para Tags, 2-3grams, Keys    \n",
    "    luego agrego estas nuevas features al embedding, con eso puedo entrenar el clasificador.          \n",
    "  \n",
    "-segundo usando topic modeling q todavia no investigue     \n",
    "\n",
    "\n",
    "Orden:  \n",
    "Crear Espacio,  \n",
    "normalizarlo,  \n",
    "reducirlo,  \n",
    "hacerle kmeans,  \n",
    "obtener los clusters,  \n",
    "medir siluete, gap, inertia, y precision con mi database.  \n",
    "Agregar la medicion al df de mediciones.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Q</th>\n",
       "      <th>Qclean</th>\n",
       "      <th>Qtoks</th>\n",
       "      <th>Qstp</th>\n",
       "      <th>Qkeys</th>\n",
       "      <th>ANS</th>\n",
       "      <th>Aclean</th>\n",
       "      <th>Atoks</th>\n",
       "      <th>Astp</th>\n",
       "      <th>Akeys</th>\n",
       "      <th>QA-keys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ojjgvy</td>\n",
       "      <td>people that eat spicy food, why?</td>\n",
       "      <td>people that eat spicy food why</td>\n",
       "      <td>[people, that, eat, spicy, food, why]</td>\n",
       "      <td>[that, why]</td>\n",
       "      <td>[people, eat, spicy, food]</td>\n",
       "      <td>I never liked spicy food, it just adds a pain ...</td>\n",
       "      <td>I never liked spicy food it just adds a pain e...</td>\n",
       "      <td>['I', 'never', 'liked', 'spicy', 'food', 'it',...</td>\n",
       "      <td>['it', 'just', 'a', 'that', 'i']</td>\n",
       "      <td>['I', 'never', 'liked', 'spicy', 'food', 'add'...</td>\n",
       "      <td>['people', 'eat', 'spicy', 'food', 'I', 'never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc60pk</td>\n",
       "      <td>donut or doughnut, &amp; where are you from?</td>\n",
       "      <td>donut or doughnut  where are you from</td>\n",
       "      <td>[donut, or, doughnut, where, are, you, from]</td>\n",
       "      <td>[or, where, are, you, from]</td>\n",
       "      <td>[donut, doughnut]</td>\n",
       "      <td>donut, southern usa</td>\n",
       "      <td>donut southern usa</td>\n",
       "      <td>['donut', 'southern', 'usa']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['donut', 'southern', 'usa']</td>\n",
       "      <td>['donut', 'doughnut', 'donut', 'southern', 'usa']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289mmm</td>\n",
       "      <td>help finding a song from a video description?</td>\n",
       "      <td>help finding a song from a video description</td>\n",
       "      <td>[help, finding, a, song, from, a, video, descr...</td>\n",
       "      <td>[a, from, a]</td>\n",
       "      <td>[help, finding, song, video, description]</td>\n",
       "      <td>/r/tipofmytongue</td>\n",
       "      <td>rtipofmytongue</td>\n",
       "      <td>['rtipofmytongue']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['rtipofmytongue']</td>\n",
       "      <td>['help', 'finding', 'song', 'video', 'descript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rufpr/</td>\n",
       "      <td>i was rapedno, we had sex</td>\n",
       "      <td>i was rapedno we had sex</td>\n",
       "      <td>[i, wa, rapedno, we, had, sex]</td>\n",
       "      <td>[i, we, had]</td>\n",
       "      <td>[wa, rapedno, sex]</td>\n",
       "      <td>if its not a yes, its no.</td>\n",
       "      <td>if its not a yes its no</td>\n",
       "      <td>['if', 'it', 'not', 'a', 'yes', 'it', 'no']</td>\n",
       "      <td>['if', 'it', 'not', 'a', 'it', 'no']</td>\n",
       "      <td>['yes']</td>\n",
       "      <td>['wa', 'rapedno', 'sex', 'yes']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24vtw3</td>\n",
       "      <td>jesus h. christ what's his middle name?</td>\n",
       "      <td>jesus h christ whats his middle name</td>\n",
       "      <td>[jesus, h, christ, whats, his, middle, name]</td>\n",
       "      <td>[his]</td>\n",
       "      <td>[jesus, h, christ, whats, middle, name]</td>\n",
       "      <td>umm... Holy. As in Holy Christ. Isn't this com...</td>\n",
       "      <td>umm Holy As in Holy Christ Isnt this common kn...</td>\n",
       "      <td>['umm', 'Holy', 'As', 'in', 'Holy', 'Christ', ...</td>\n",
       "      <td>['in', 'this']</td>\n",
       "      <td>['umm', 'Holy', 'As', 'Holy', 'Christ', 'Isnt'...</td>\n",
       "      <td>['jesus', 'h', 'christ', 'whats', 'middle', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90149</th>\n",
       "      <td>oqcloa</td>\n",
       "      <td>¿what easy work is really difficult?</td>\n",
       "      <td>¿what easy work is really difficult</td>\n",
       "      <td>[¿what, easy, work, is, really, difficult]</td>\n",
       "      <td>[¿what, is]</td>\n",
       "      <td>[¿, easy, work, really, difficult]</td>\n",
       "      <td>None by virtue of it being easy in the first p...</td>\n",
       "      <td>None by virtue of it being easy in the first p...</td>\n",
       "      <td>['None', 'by', 'virtue', 'of', 'it', 'being', ...</td>\n",
       "      <td>['by', 'of', 'it', 'being', 'in', 'the']</td>\n",
       "      <td>['None', 'virtue', 'easy', 'first', 'place']</td>\n",
       "      <td>['¿', 'easy', 'work', 'really', 'difficult', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90150</th>\n",
       "      <td>ooswef</td>\n",
       "      <td>daddy what does nsfw mean?</td>\n",
       "      <td>daddy what does nsfw mean</td>\n",
       "      <td>[daddy, what, doe, nsfw, mean]</td>\n",
       "      <td>[what]</td>\n",
       "      <td>[daddy, doe, nsfw, mean]</td>\n",
       "      <td>not safe for work</td>\n",
       "      <td>not safe for work</td>\n",
       "      <td>['not', 'safe', 'for', 'work']</td>\n",
       "      <td>['not', 'for']</td>\n",
       "      <td>['safe', 'work']</td>\n",
       "      <td>['daddy', 'doe', 'nsfw', 'mean', 'safe', 'work']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90151</th>\n",
       "      <td>orgpc4</td>\n",
       "      <td>quiet people, what are your strengths?</td>\n",
       "      <td>quiet people what are your strengths</td>\n",
       "      <td>[quiet, people, what, are, your, strength]</td>\n",
       "      <td>[what, are, your]</td>\n",
       "      <td>[quiet, people, strength]</td>\n",
       "      <td>Observing, attention, memory,</td>\n",
       "      <td>Observing attention memory</td>\n",
       "      <td>['Observing', 'attention', 'memory']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Observing', 'attention', 'memory']</td>\n",
       "      <td>['quiet', 'people', 'strength', 'Observing', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90152</th>\n",
       "      <td>og7m7k</td>\n",
       "      <td>what is the best indian movie?</td>\n",
       "      <td>what is the best indian movie</td>\n",
       "      <td>[what, is, the, best, indian, movie]</td>\n",
       "      <td>[what, is, the]</td>\n",
       "      <td>[best, indian, movie]</td>\n",
       "      <td>Why is this a quote?</td>\n",
       "      <td>Why is this a quote</td>\n",
       "      <td>['Why', 'is', 'this', 'a', 'quote']</td>\n",
       "      <td>['is', 'this', 'a']</td>\n",
       "      <td>['Why', 'quote']</td>\n",
       "      <td>['best', 'indian', 'movie', 'Why', 'quote']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90153</th>\n",
       "      <td>olnost</td>\n",
       "      <td>who's the best mc's, biggie, jay-z, and nas?</td>\n",
       "      <td>whos the best mcs biggie jayz and nas</td>\n",
       "      <td>[who, the, best, mc, biggie, jayz, and, na]</td>\n",
       "      <td>[who, the, and]</td>\n",
       "      <td>[best, mc, biggie, jayz, na]</td>\n",
       "      <td>whyd you put your question in quotes?</td>\n",
       "      <td>whyd you put your question in quotes</td>\n",
       "      <td>['whyd', 'you', 'put', 'your', 'question', 'in...</td>\n",
       "      <td>['you', 'your', 'in']</td>\n",
       "      <td>['whyd', 'put', 'question', 'quote']</td>\n",
       "      <td>['best', 'mc', 'biggie', 'jayz', 'na', 'whyd',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90154 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              Q  \\\n",
       "0      ojjgvy               people that eat spicy food, why?   \n",
       "1      bc60pk       donut or doughnut, & where are you from?   \n",
       "2      289mmm  help finding a song from a video description?   \n",
       "3      rufpr/                      i was rapedno, we had sex   \n",
       "4      24vtw3        jesus h. christ what's his middle name?   \n",
       "...       ...                                            ...   \n",
       "90149  oqcloa           ¿what easy work is really difficult?   \n",
       "90150  ooswef                     daddy what does nsfw mean?   \n",
       "90151  orgpc4         quiet people, what are your strengths?   \n",
       "90152  og7m7k                 what is the best indian movie?   \n",
       "90153  olnost   who's the best mc's, biggie, jay-z, and nas?   \n",
       "\n",
       "                                             Qclean  \\\n",
       "0                    people that eat spicy food why   \n",
       "1             donut or doughnut  where are you from   \n",
       "2      help finding a song from a video description   \n",
       "3                          i was rapedno we had sex   \n",
       "4              jesus h christ whats his middle name   \n",
       "...                                             ...   \n",
       "90149           ¿what easy work is really difficult   \n",
       "90150                     daddy what does nsfw mean   \n",
       "90151          quiet people what are your strengths   \n",
       "90152                 what is the best indian movie   \n",
       "90153         whos the best mcs biggie jayz and nas   \n",
       "\n",
       "                                                   Qtoks  \\\n",
       "0                  [people, that, eat, spicy, food, why]   \n",
       "1           [donut, or, doughnut, where, are, you, from]   \n",
       "2      [help, finding, a, song, from, a, video, descr...   \n",
       "3                         [i, wa, rapedno, we, had, sex]   \n",
       "4           [jesus, h, christ, whats, his, middle, name]   \n",
       "...                                                  ...   \n",
       "90149         [¿what, easy, work, is, really, difficult]   \n",
       "90150                     [daddy, what, doe, nsfw, mean]   \n",
       "90151         [quiet, people, what, are, your, strength]   \n",
       "90152               [what, is, the, best, indian, movie]   \n",
       "90153        [who, the, best, mc, biggie, jayz, and, na]   \n",
       "\n",
       "                              Qstp                                      Qkeys  \\\n",
       "0                      [that, why]                 [people, eat, spicy, food]   \n",
       "1      [or, where, are, you, from]                          [donut, doughnut]   \n",
       "2                     [a, from, a]  [help, finding, song, video, description]   \n",
       "3                     [i, we, had]                         [wa, rapedno, sex]   \n",
       "4                            [his]    [jesus, h, christ, whats, middle, name]   \n",
       "...                            ...                                        ...   \n",
       "90149                  [¿what, is]         [¿, easy, work, really, difficult]   \n",
       "90150                       [what]                   [daddy, doe, nsfw, mean]   \n",
       "90151            [what, are, your]                  [quiet, people, strength]   \n",
       "90152              [what, is, the]                      [best, indian, movie]   \n",
       "90153              [who, the, and]               [best, mc, biggie, jayz, na]   \n",
       "\n",
       "                                                     ANS  \\\n",
       "0      I never liked spicy food, it just adds a pain ...   \n",
       "1                                    donut, southern usa   \n",
       "2                                       /r/tipofmytongue   \n",
       "3                              if its not a yes, its no.   \n",
       "4      umm... Holy. As in Holy Christ. Isn't this com...   \n",
       "...                                                  ...   \n",
       "90149  None by virtue of it being easy in the first p...   \n",
       "90150                                  not safe for work   \n",
       "90151                      Observing, attention, memory,   \n",
       "90152                               Why is this a quote?   \n",
       "90153              whyd you put your question in quotes?   \n",
       "\n",
       "                                                  Aclean  \\\n",
       "0      I never liked spicy food it just adds a pain e...   \n",
       "1                                     donut southern usa   \n",
       "2                                         rtipofmytongue   \n",
       "3                                if its not a yes its no   \n",
       "4      umm Holy As in Holy Christ Isnt this common kn...   \n",
       "...                                                  ...   \n",
       "90149  None by virtue of it being easy in the first p...   \n",
       "90150                                  not safe for work   \n",
       "90151                         Observing attention memory   \n",
       "90152                                Why is this a quote   \n",
       "90153               whyd you put your question in quotes   \n",
       "\n",
       "                                                   Atoks  \\\n",
       "0      ['I', 'never', 'liked', 'spicy', 'food', 'it',...   \n",
       "1                           ['donut', 'southern', 'usa']   \n",
       "2                                     ['rtipofmytongue']   \n",
       "3            ['if', 'it', 'not', 'a', 'yes', 'it', 'no']   \n",
       "4      ['umm', 'Holy', 'As', 'in', 'Holy', 'Christ', ...   \n",
       "...                                                  ...   \n",
       "90149  ['None', 'by', 'virtue', 'of', 'it', 'being', ...   \n",
       "90150                     ['not', 'safe', 'for', 'work']   \n",
       "90151               ['Observing', 'attention', 'memory']   \n",
       "90152                ['Why', 'is', 'this', 'a', 'quote']   \n",
       "90153  ['whyd', 'you', 'put', 'your', 'question', 'in...   \n",
       "\n",
       "                                           Astp  \\\n",
       "0              ['it', 'just', 'a', 'that', 'i']   \n",
       "1                                            []   \n",
       "2                                            []   \n",
       "3          ['if', 'it', 'not', 'a', 'it', 'no']   \n",
       "4                                ['in', 'this']   \n",
       "...                                         ...   \n",
       "90149  ['by', 'of', 'it', 'being', 'in', 'the']   \n",
       "90150                            ['not', 'for']   \n",
       "90151                                        []   \n",
       "90152                       ['is', 'this', 'a']   \n",
       "90153                     ['you', 'your', 'in']   \n",
       "\n",
       "                                                   Akeys  \\\n",
       "0      ['I', 'never', 'liked', 'spicy', 'food', 'add'...   \n",
       "1                           ['donut', 'southern', 'usa']   \n",
       "2                                     ['rtipofmytongue']   \n",
       "3                                                ['yes']   \n",
       "4      ['umm', 'Holy', 'As', 'Holy', 'Christ', 'Isnt'...   \n",
       "...                                                  ...   \n",
       "90149       ['None', 'virtue', 'easy', 'first', 'place']   \n",
       "90150                                   ['safe', 'work']   \n",
       "90151               ['Observing', 'attention', 'memory']   \n",
       "90152                                   ['Why', 'quote']   \n",
       "90153               ['whyd', 'put', 'question', 'quote']   \n",
       "\n",
       "                                                 QA-keys  \n",
       "0      ['people', 'eat', 'spicy', 'food', 'I', 'never...  \n",
       "1      ['donut', 'doughnut', 'donut', 'southern', 'usa']  \n",
       "2      ['help', 'finding', 'song', 'video', 'descript...  \n",
       "3                        ['wa', 'rapedno', 'sex', 'yes']  \n",
       "4      ['jesus', 'h', 'christ', 'whats', 'middle', 'n...  \n",
       "...                                                  ...  \n",
       "90149  ['¿', 'easy', 'work', 'really', 'difficult', '...  \n",
       "90150   ['daddy', 'doe', 'nsfw', 'mean', 'safe', 'work']  \n",
       "90151  ['quiet', 'people', 'strength', 'Observing', '...  \n",
       "90152        ['best', 'indian', 'movie', 'Why', 'quote']  \n",
       "90153  ['best', 'mc', 'biggie', 'jayz', 'na', 'whyd',...  \n",
       "\n",
       "[90154 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fasttext\n",
    "from patterns import emoji_pattern\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"db/features.csv\", index_col=0, converters={'Qtoks': pd.eval,'Qstp': pd.eval,'Qkeys': pd.eval})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "podria ser capa de LSA? (a los embedings tmb?   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = {}\n",
    "gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdidf_model(df, column, range=(2,3), min_df=20, threshold=1e-3, ngram=False):\n",
    "    \n",
    "    docs = df[column].fillna(\"\")\n",
    "\n",
    "    if ngram:\n",
    "        tdidf = TfidfVectorizer(min_df=min_df, ngram_range=range)\n",
    "    else:\n",
    "        tdidf = TfidfVectorizer(min_df=min_df)\n",
    "\n",
    "    #Create, Normalize and Reduce \n",
    "    model = tdidf.fit_transform(docs)\n",
    "    model = normalize(model, axis=1, norm=\"max\")\n",
    "    model = VarianceThreshold(threshold=threshold).fit_transform(model)\n",
    "\n",
    "    return np.array(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Qkeys nonstop lemmas\n",
    "df[\"Q-kstr\"] =  df[\"Qkeys\"].apply(lambda x: \" \".join(a for a in x))\n",
    "df[\"QA-kstr\"] =  df[\"Qkeys\"].apply(lambda x: \" \".join(a for a in x))\n",
    "df[\"Q-stpstr\"] =  df[\"Qkeys\"].apply(lambda x: \" \".join(a for a in x))\n",
    "\n",
    "\n",
    "Models[\"Q-keys\"] = tdidf_model(df, column=\"Q-kstr\", min_df=15)\n",
    "\n",
    "#Qclean is without punct\n",
    "Models[\"Q-ngrams\"] = tdidf_model(df, column=\"Qclean\", range=(2,3), min_df=15, ngram = True)\n",
    "\n",
    "#Question+Answer nonstop lemmas\n",
    "Models[\"QA-keys\"] = tdidf_model(df, column=\"QA-kstr\", min_df=15)\n",
    "\n",
    "#i think it can help tu\n",
    "Models[\"Q-stop\"] = tdidf_model(df, column=\"Q-stpstr\", min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "https://kavita-ganesan.com/fasttext-vs-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def w2v_doc_embedding(docs_toks):\n",
    "\n",
    "    docs_model = []\n",
    "    \n",
    "    words_model = Word2Vec(docs_toks, size=300, workers=8, seed=0)\n",
    "\n",
    "    for tokens in docs_toks:\n",
    "        zero_vector = np.zeros(words_model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in words_model.wv:\n",
    "                try:\n",
    "                    vectors.append(words_model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            docs_model.append(avg_vec)\n",
    "        else:\n",
    "            docs_model.append(zero_vector)\n",
    "    return np.array(docs_model)\n",
    "\n",
    "Models[\"w2v-Q\"] = w2v_doc_embedding(list(df[\"Qkeys\"]))\n",
    "Models[\"w2v-QA\"] = w2v_doc_embedding(list(df[\"QA-keys\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Embeddings\n",
    "\n",
    "- SBERT , glove-6B-300\n",
    "    * Q, QLess, QA, QAless  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    glove60 = KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=False, no_header=True)\n",
    "    def glove_doc_embedding(docs_toks, words_model):\n",
    "\n",
    "        docs_model = []\n",
    "        \n",
    "\n",
    "        for tokens in docs_toks:\n",
    "            zero_vector = np.zeros(words_model.vector_size)\n",
    "            vectors = []\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    vectors.append(words_model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                docs_model.append(avg_vec)\n",
    "            else:\n",
    "                docs_model.append(zero_vector)\n",
    "        return docs_model\n",
    "        \n",
    "    Models[\"glove-Q\"] = glove_doc_embedding(list(df[\"Qtoks\"]), glove60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model = SentenceTransformer('nreimers/MiniLM-L6-H384-uncased') #\n",
    "    Models[\"SBERT\"] = model.encode(list(df[\"Q\"].astype('str')), batch_size=70, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "https://github.com/ddangelov/Top2Vec\n",
    "https://pythonrepo.com/repo/ddangelov-Top2Vec-python-natural-language-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "if gpu:\n",
    "    Models[\"top2vec\"] = Top2Vec(list(df[\"Q\"].astype('str')), embedding_model='universal-sentence-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.007*\"after\" + 0.007*\"them\" + 0.007*\"him\" + 0.007*\"having\" + 0.007*\"once\" + 0.007*\"s\" + 0.007*\"shouldn\" + 0.007*\"out\" + 0.007*\"through\" + 0.007*\"isn\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.007*\"her\" + 0.007*\"against\" + 0.007*\"on\" + 0.007*\"few\" + 0.007*\"hot…why\" + 0.007*\"not\" + 0.007*\"been\" + 0.007*\"we\" + 0.007*\"he\" + 0.007*\"how\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.008*\"under\" + 0.008*\"very\" + 0.008*\"home…what\" + 0.007*\"those\" + 0.007*\"was…\" + 0.007*\"being\" + 0.007*\"any\" + 0.007*\"whom\" + 0.007*\"against\" + 0.007*\"disappointment—and\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.008*\"it\" + 0.008*\"about\" + 0.008*\"all\" + 0.007*\"as\" + 0.007*\"your\" + 0.007*\"whom\" + 0.007*\"after\" + 0.007*\"over\" + 0.007*\"too\" + 0.007*\"o\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.008*\"gossip—why\" + 0.008*\"a…\" + 0.007*\"aren\" + 0.007*\"again\" + 0.007*\"than\" + 0.007*\"we…\" + 0.007*\"shouldn\" + 0.007*\"won\" + 0.007*\"in\" + 0.007*\"after\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.007*\"be\" + 0.007*\"if\" + 0.007*\"should\" + 0.007*\"yourself\" + 0.007*\"have\" + 0.007*\"too\" + 0.007*\"home…what\" + 0.007*\"did\" + 0.007*\"disappointment—and\" + 0.007*\"during\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.007*\"i\" + 0.007*\"herself\" + 0.007*\"what´s\" + 0.007*\"my\" + 0.007*\"while\" + 0.007*\"weren\" + 0.007*\"any\" + 0.007*\"on\" + 0.007*\"those\" + 0.007*\"same\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.009*\"those\" + 0.007*\"was…\" + 0.007*\"that\" + 0.007*\"disappointment—and\" + 0.007*\"can\" + 0.007*\"once\" + 0.007*\"some\" + 0.007*\"haven\" + 0.007*\"when……\" + 0.007*\"were\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.008*\"too\" + 0.007*\"which\" + 0.007*\"aren\" + 0.007*\"yours\" + 0.007*\"all\" + 0.007*\"myself\" + 0.007*\"been\" + 0.007*\"from\" + 0.007*\"is\" + 0.007*\"only—\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.007*\"asked…why\" + 0.007*\"wouldn\" + 0.007*\"when……\" + 0.007*\"ain\" + 0.007*\"my\" + 0.007*\"hot…why\" + 0.007*\"for\" + 0.007*\"who\" + 0.007*\"we\" + 0.007*\"where\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.008*\"am\" + 0.007*\"those\" + 0.007*\"few\" + 0.007*\"below\" + 0.007*\"just\" + 0.007*\"his\" + 0.007*\"than\" + 0.007*\"out\" + 0.007*\"while\" + 0.007*\"by\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.007*\"most\" + 0.007*\"nor\" + 0.007*\"¿what\" + 0.007*\"just\" + 0.007*\"him\" + 0.007*\"again\" + 0.007*\"but\" + 0.007*\"row…what\" + 0.007*\"i\" + 0.007*\"from\"\n",
      "\n",
      "\n",
      "Topic: 12 \n",
      "Words: 0.008*\"what´s\" + 0.007*\"more…corn\" + 0.007*\"it\" + 0.007*\"isn\" + 0.007*\"row…what\" + 0.007*\"during\" + 0.007*\"her\" + 0.007*\"after\" + 0.007*\"and\" + 0.007*\"from\"\n",
      "\n",
      "\n",
      "Topic: 13 \n",
      "Words: 0.008*\"but\" + 0.008*\"couldn\" + 0.008*\"s\" + 0.008*\"a\" + 0.007*\"up\" + 0.007*\"home…what\" + 0.007*\"weren\" + 0.007*\"aren\" + 0.007*\"off\" + 0.007*\"for…\"\n",
      "\n",
      "\n",
      "Topic: 14 \n",
      "Words: 0.007*\"herself\" + 0.007*\"for\" + 0.007*\"s\" + 0.007*\"ve\" + 0.007*\"the\" + 0.007*\"those\" + 0.007*\"both\" + 0.007*\"under\" + 0.007*\"shouldn\" + 0.007*\"during\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(df[\"Qstp\"])\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in df[\"Qtoks\"]]\n",
    "\n",
    "Models[\"LDA-keys\"] = LdaMulticore(list(), \n",
    "                                   num_topics = 15,\n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 80,\n",
    "                                   workers = 8)\n",
    "\n",
    "for idx, topic in Models[\"LDA-keys\"].print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo voy a probar con KMeans por cuestion de cantidad de datos y recursos, lo que me permite\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html\n",
    "https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92  \n",
    "https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29  \n",
    "https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html   s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "\n",
    "class FaissKMeans:\n",
    "    def __init__(self, n_clus=10, n_init=10, max_iter=50):\n",
    "        self.n_clusters = n_clus\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "        self.kmeans = None\n",
    "        self.cluster_centers_ = None\n",
    "        self.inertia_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.kmeans = faiss.Kmeans(d=X.shape[1],\n",
    "                                   k=self.n_clusters,\n",
    "                                   niter=self.max_iter,\n",
    "                                   nredo=self.n_init)\n",
    "        self.kmeans.train(X.astype(np.float32))\n",
    "        self.cluster_centers_ = self.kmeans.centroids\n",
    "        self.inertia_ = self.kmeans.obj[-1]\n",
    "        #print(self.kmeans.obj)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.kmeans.index.search(X.astype(np.float32), 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "inertia, silhuete, own_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con este dataset el metodo de elbow no se puede utiizar porque el error crece linearmente\n",
    "def get_elbow(X,ran=(8,20)):\n",
    "    #from kneed import KneeLocator, DataGenerator\n",
    "\n",
    "    distortions = []\n",
    "    for i in range(1000,1003):\n",
    "        fkm = FaissKMeans(n_clus=i)\n",
    "        fkm.fit(X, [])\n",
    "        distortions.append(fkm.inertia_)\n",
    "    return distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# n_clusters=30\n",
    "# kmc = KMeans(n_clusters=n_clusters,  random_state=0)\n",
    "# kmc.fit(X=Models[\"top2vec\"])\n",
    "\n",
    "# df[\"label1\"] = kmc.labels_\n",
    "\n",
    "#vemos los primeros clusters, al ser con ngrams  la forma en la que comienza la pregunta tiene mucho peso\n",
    "def print_kmc_clus(n=10):\n",
    "    import random\n",
    "    q_clus = [[] for i in range(n_clusters)]\n",
    "\n",
    "    for sentence_id, cluster_id in enumerate(kmc.labels_):\n",
    "        q_clus[cluster_id].append(df.iloc[sentence_id].Q)\n",
    "\n",
    "    for i, cluster in enumerate(q_clus):\n",
    "        print(f\"Cluster {i+1}, len: {len(cluster)}\" )\n",
    "        for tw in random.sample(cluster, n):\n",
    "            print(tw)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "#print_kmc_clus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Compara la cantidad de aciertos con los df de test\n",
    "def check_results():\n",
    "    test_df = pd.read_csv(\"db/test_db.csv\", index_col=0)\n",
    "    true_df = test_df[test_df[\"2\"]]\n",
    "    false_df = test_df[test_df[\"2\"] == False]\n",
    "\n",
    "    n = len(true_df)\n",
    "    m = len(false_df)\n",
    "    same_c = 0\n",
    "    for i in range(n):\n",
    "        if int(df.loc[true_df.iloc[i][0]].label1) ==  int(df.loc[true_df.iloc[i][1]].label1):\n",
    "            same_c += 1\n",
    "    \n",
    "    diff_c = 0\n",
    "    for i in range(m):\n",
    "        if int(df.loc[false_df.iloc[i][0]].label1) !=  int(df.loc[false_df.iloc[i][1]].label1):\n",
    "            diff_c += 1\n",
    "        \n",
    "    return same_c/n, diff_c/m\n",
    "        \n",
    "#check_results()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
