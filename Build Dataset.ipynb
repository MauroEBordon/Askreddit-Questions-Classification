{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Dataset Creation\n","\n","Corpus from here: [kaggle.com/mauroebordon/creating-a-qa-corpus-from-askreddit/](https://www.kaggle.com/mauroebordon/creating-a-qa-corpus-from-askreddit/)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import re\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","\n","db_df = pd.read_csv(\"db/ask-reddit-corpus.csv\", index_col=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Extraction\n","\n","Only Tokens, Lemmas, PoS AND stopword filtering for now.\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","def extract_features(df: pd.DataFrame, toks=True, lems=True, pos=True):\n","\n","    \"\"\"\n","    idea: las collocations\n","    \"\"\"\n","    lmtzr = WordNetLemmatizer()\n","   \n","    #Filtramos las preguntas demasiado extensas\n","    db_df = df.copy()[df.Q.apply(lambda x: len(str(x)) <50)]\n","\n","    #remove stopwords\n","    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')        \n","    db_df[\"Qless\"] = db_df.Q.str.replace(pattern, '')\n","    db_df[\"Aless\"] = db_df.ANS.str.replace(pattern, '')\n","        \n","    #Obtenemos los tokens\n","    if toks:\n","        db_df[\"Qtoks\"] = [word_tokenize(w) for w in db_df[\"Qless\"]]\n","        db_df[\"Atoks\"] = [word_tokenize(w) for w in db_df[\"Aless\"]]\n","\n","    #Obtenemos los lemmas\n","    if lems:\n","        db_df[\"Qlemmas\"] = [' '.join(lmtzr.lemmatize(t) for t in qes) for qes in db_df[\"Qtoks\"]]\n","        db_df[\"Alemmas\"] = [' '.join(lmtzr.lemmatize(t) for t in qes) for qes in db_df[\"Atoks\"]]\n","\n","\n","    # Par Tok & POS\n","    if pos:\n","        db_df[\"Qpos\"] = [pos_tag(word_tokenize(w)) for w in db_df[\"Qless\"]]\n","        db_df[\"Apos\"] = [pos_tag(word_tokenize(w)) for w in db_df[\"Aless\"]]\n","\n","    \n","    \n","    db_df = db_df[[\"id\", \"Q\", \"Qscore\", \"Qless\", \"Qlemmas\", \"Qpos\", \"Qtoks\", \"ANS\", \"ANSscore\", \"Aless\", \"Alemmas\", \"Apos\", \"Atoks\"]]\n","\n","    return db_df\n","    \n","\n","#usar pickle no sirve para comprimir naranja\n","df = extract_features(db_df)"]},{"cell_type":"markdown","metadata":{},"source":["## Further Filtering\n","\n","como tiene cerca de ~100k entradas vamos a filtrar un poco "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#solo consideramos con las preguntas con 2 o mÃ¡s upvotes (~32k)\n","sdf = df[df.Qscore > 1]\n","\n","#filtramos las respuestas muy cortas, nos deja un total de 25k. \n","sdf = sdf.copy()[sdf.ANS.apply(lambda x: len(str(x)) > 15)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.to_csv(\"db/features.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
