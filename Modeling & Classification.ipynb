{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "~~usar varios clusters, uno de 3grams, uno de nouns, uno de embeddings~~   \n",
    "~~con esto buscar un clasificador multilabel (o simplemente agregarlos como features del modelo que le fedee al clasificador)~~  \n",
    "\n",
    "Des métodos para clusterizar:    \n",
    "-primero hago cluster separados para Tags, 2-3grams, Keys    \n",
    "    luego agrego estas nuevas features al embedding, con eso puedo entrenar el clasificador.          \n",
    "  \n",
    "-segundo usando topic modeling q todavia no investigue     \n",
    "\n",
    "\n",
    "Orden:  \n",
    "Crear Espacio,  \n",
    "normalizarlo,  \n",
    "reducirlo,  \n",
    "hacerle kmeans,  \n",
    "obtener los clusters,  \n",
    "medir siluete, gap, inertia, y precision con mi database.  \n",
    "Agregar la medicion al df de mediciones.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Q</th>\n",
       "      <th>Qless</th>\n",
       "      <th>Qlemmas</th>\n",
       "      <th>Qpos</th>\n",
       "      <th>Qkeys</th>\n",
       "      <th>Qclean</th>\n",
       "      <th>ANS</th>\n",
       "      <th>Aless</th>\n",
       "      <th>Alemmas</th>\n",
       "      <th>Apos</th>\n",
       "      <th>Akeys</th>\n",
       "      <th>Aclean</th>\n",
       "      <th>QA-keys</th>\n",
       "      <th>QA-lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ojjgvy</td>\n",
       "      <td>people that eat spicy food, why?</td>\n",
       "      <td>people eat spicy food</td>\n",
       "      <td>['people', 'eat', 'spicy', 'food']</td>\n",
       "      <td>[('people', 'NNS'), ('eat', 'VBP'), ('spicy', ...</td>\n",
       "      <td>people_NNS eat_VBP spicy_NN food_NN</td>\n",
       "      <td>people that eat spicy food why</td>\n",
       "      <td>I never liked spicy food, it just adds a pain ...</td>\n",
       "      <td>I never liked spicy food adds pain element don...</td>\n",
       "      <td>['I', 'never', 'liked', 'spicy', 'food', 'add'...</td>\n",
       "      <td>[('I', 'PRP'), ('never', 'RB'), ('liked', 'VBD...</td>\n",
       "      <td>I_PRP never_RB liked_VBD spicy_NN food_NN add_...</td>\n",
       "      <td>I never liked spicy food it just adds a pain e...</td>\n",
       "      <td>people_NNS eat_VBP spicy_NN food_NN  I_PRP nev...</td>\n",
       "      <td>['people', 'eat', 'spicy', 'food', 'I', 'never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bc60pk</td>\n",
       "      <td>donut or doughnut, &amp; where are you from?</td>\n",
       "      <td>donut doughnut</td>\n",
       "      <td>['donut', 'doughnut']</td>\n",
       "      <td>[('donut', 'NN'), ('doughnut', 'NN')]</td>\n",
       "      <td>donut_NN doughnut_NN</td>\n",
       "      <td>donut or doughnut  where are you from</td>\n",
       "      <td>donut, southern usa</td>\n",
       "      <td>donut southern usa</td>\n",
       "      <td>['donut', 'southern', 'usa']</td>\n",
       "      <td>[('donut', 'NN'), ('southern', 'JJ'), ('usa', ...</td>\n",
       "      <td>donut_NN usa_NN</td>\n",
       "      <td>donut southern usa</td>\n",
       "      <td>donut_NN doughnut_NN  donut_NN usa_NN</td>\n",
       "      <td>['donut', 'doughnut', 'donut', 'southern', 'usa']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289mmm</td>\n",
       "      <td>help finding a song from a video description?</td>\n",
       "      <td>help finding song video description</td>\n",
       "      <td>['help', 'finding', 'song', 'video', 'descript...</td>\n",
       "      <td>[('help', 'NN'), ('finding', 'NN'), ('song', '...</td>\n",
       "      <td>help_NN finding_NN song_NN video_NN descriptio...</td>\n",
       "      <td>help finding a song from a video description</td>\n",
       "      <td>/r/tipofmytongue</td>\n",
       "      <td>rtipofmytongue</td>\n",
       "      <td>['rtipofmytongue']</td>\n",
       "      <td>[('rtipofmytongue', 'NN')]</td>\n",
       "      <td>rtipofmytongue_NN</td>\n",
       "      <td>rtipofmytongue</td>\n",
       "      <td>help_NN finding_NN song_NN video_NN descriptio...</td>\n",
       "      <td>['help', 'finding', 'song', 'video', 'descript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rufpr/</td>\n",
       "      <td>i was rapedno, we had sex</td>\n",
       "      <td>rapedno sex</td>\n",
       "      <td>['rapedno', 'sex']</td>\n",
       "      <td>[('rapedno', 'NN'), ('sex', 'NN')]</td>\n",
       "      <td>rapedno_NN sex_NN</td>\n",
       "      <td>i was rapedno we had sex</td>\n",
       "      <td>if its not a yes, its no.</td>\n",
       "      <td>yes</td>\n",
       "      <td>['yes']</td>\n",
       "      <td>[('yes', 'NNS')]</td>\n",
       "      <td>yes_NNS</td>\n",
       "      <td>if its not a yes its no</td>\n",
       "      <td>rapedno_NN sex_NN  yes_NNS</td>\n",
       "      <td>['rapedno', 'sex', 'yes']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24vtw3</td>\n",
       "      <td>jesus h. christ what's his middle name?</td>\n",
       "      <td>jesus h christ whats middle name</td>\n",
       "      <td>['jesus', 'h', 'christ', 'whats', 'middle', 'n...</td>\n",
       "      <td>[('jesus', 'NN'), ('h', 'NN'), ('christ', 'NN'...</td>\n",
       "      <td>jesus_NN h_NN christ_NN whats_NNS middle_VBP n...</td>\n",
       "      <td>jesus h christ whats his middle name</td>\n",
       "      <td>umm... Holy. As in Holy Christ. Isn't this com...</td>\n",
       "      <td>umm Holy As Holy Christ Isnt common knowledge</td>\n",
       "      <td>['umm', 'Holy', 'As', 'Holy', 'Christ', 'Isnt'...</td>\n",
       "      <td>[('umm', 'JJ'), ('Holy', 'NNP'), ('As', 'IN'),...</td>\n",
       "      <td>Holy_NNP As_IN Holy_NNP Christ_NNP Isnt_NNP kn...</td>\n",
       "      <td>umm Holy As in Holy Christ Isnt this common kn...</td>\n",
       "      <td>jesus_NN h_NN christ_NN whats_NNS middle_VBP n...</td>\n",
       "      <td>['jesus', 'h', 'christ', 'whats', 'middle', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90149</th>\n",
       "      <td>oqcloa</td>\n",
       "      <td>¿what easy work is really difficult?</td>\n",
       "      <td>¿easy work really difficult</td>\n",
       "      <td>['¿easy', 'work', 'really', 'difficult']</td>\n",
       "      <td>[('¿easy', 'JJ'), ('work', 'NN'), ('really', '...</td>\n",
       "      <td>work_NN really_RB</td>\n",
       "      <td>¿what easy work is really difficult</td>\n",
       "      <td>None by virtue of it being easy in the first p...</td>\n",
       "      <td>None virtue easy first place</td>\n",
       "      <td>['None', 'virtue', 'easy', 'first', 'place']</td>\n",
       "      <td>[('None', 'NN'), ('virtue', 'NN'), ('easy', 'V...</td>\n",
       "      <td>None_NN virtue_NN easy_VBP place_NN</td>\n",
       "      <td>None by virtue of it being easy in the first p...</td>\n",
       "      <td>work_NN really_RB  None_NN virtue_NN easy_VBP ...</td>\n",
       "      <td>['¿easy', 'work', 'really', 'difficult', 'None...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90150</th>\n",
       "      <td>ooswef</td>\n",
       "      <td>daddy what does nsfw mean?</td>\n",
       "      <td>daddy nsfw mean</td>\n",
       "      <td>['daddy', 'nsfw', 'mean']</td>\n",
       "      <td>[('daddy', 'JJ'), ('nsfw', 'NNS'), ('mean', 'V...</td>\n",
       "      <td>nsfw_NNS mean_VBP</td>\n",
       "      <td>daddy what does nsfw mean</td>\n",
       "      <td>not safe for work</td>\n",
       "      <td>safe work</td>\n",
       "      <td>['safe', 'work']</td>\n",
       "      <td>[('safe', 'JJ'), ('work', 'NN')]</td>\n",
       "      <td>work_NN</td>\n",
       "      <td>not safe for work</td>\n",
       "      <td>nsfw_NNS mean_VBP  work_NN</td>\n",
       "      <td>['daddy', 'nsfw', 'mean', 'safe', 'work']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90151</th>\n",
       "      <td>orgpc4</td>\n",
       "      <td>quiet people, what are your strengths?</td>\n",
       "      <td>quiet people strengths</td>\n",
       "      <td>['quiet', 'people', 'strength']</td>\n",
       "      <td>[('quiet', 'JJ'), ('people', 'NNS'), ('strengt...</td>\n",
       "      <td>people_NNS strength_NN</td>\n",
       "      <td>quiet people what are your strengths</td>\n",
       "      <td>Observing, attention, memory,</td>\n",
       "      <td>Observing attention memory</td>\n",
       "      <td>['Observing', 'attention', 'memory']</td>\n",
       "      <td>[('Observing', 'VBG'), ('attention', 'NN'), ('...</td>\n",
       "      <td>Observing_VBG attention_NN memory_NN</td>\n",
       "      <td>Observing attention memory</td>\n",
       "      <td>people_NNS strength_NN  Observing_VBG attentio...</td>\n",
       "      <td>['quiet', 'people', 'strength', 'Observing', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90152</th>\n",
       "      <td>og7m7k</td>\n",
       "      <td>what is the best indian movie?</td>\n",
       "      <td>best indian movie</td>\n",
       "      <td>['best', 'indian', 'movie']</td>\n",
       "      <td>[('best', 'JJS'), ('indian', 'JJ'), ('movie', ...</td>\n",
       "      <td>movie_NN</td>\n",
       "      <td>what is the best indian movie</td>\n",
       "      <td>Why is this a quote?</td>\n",
       "      <td>Why quote</td>\n",
       "      <td>['Why', 'quote']</td>\n",
       "      <td>[('Why', 'WRB'), ('quote', 'NN')]</td>\n",
       "      <td>Why_WRB quote_NN</td>\n",
       "      <td>Why is this a quote</td>\n",
       "      <td>movie_NN  Why_WRB quote_NN</td>\n",
       "      <td>['best', 'indian', 'movie', 'Why', 'quote']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90153</th>\n",
       "      <td>olnost</td>\n",
       "      <td>who's the best mc's, biggie, jay-z, and nas?</td>\n",
       "      <td>whos best mcs biggie jayz nas</td>\n",
       "      <td>['who', 'best', 'mc', 'biggie', 'jayz', 'na']</td>\n",
       "      <td>[('who', 'WP'), ('best', 'JJS'), ('mc', 'NN'),...</td>\n",
       "      <td>who_WP mc_NN biggie_NN jayz_NN na_NN</td>\n",
       "      <td>whos the best mcs biggie jayz and nas</td>\n",
       "      <td>whyd you put your question in quotes?</td>\n",
       "      <td>whyd put question quotes</td>\n",
       "      <td>['whyd', 'put', 'question', 'quote']</td>\n",
       "      <td>[('whyd', 'NN'), ('put', 'VBD'), ('question', ...</td>\n",
       "      <td>whyd_NN put_VBD question_NN quote_NN</td>\n",
       "      <td>whyd you put your question in quotes</td>\n",
       "      <td>who_WP mc_NN biggie_NN jayz_NN na_NN  whyd_NN ...</td>\n",
       "      <td>['who', 'best', 'mc', 'biggie', 'jayz', 'na', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90154 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              Q  \\\n",
       "0      ojjgvy               people that eat spicy food, why?   \n",
       "1      bc60pk       donut or doughnut, & where are you from?   \n",
       "2      289mmm  help finding a song from a video description?   \n",
       "3      rufpr/                      i was rapedno, we had sex   \n",
       "4      24vtw3        jesus h. christ what's his middle name?   \n",
       "...       ...                                            ...   \n",
       "90149  oqcloa           ¿what easy work is really difficult?   \n",
       "90150  ooswef                     daddy what does nsfw mean?   \n",
       "90151  orgpc4         quiet people, what are your strengths?   \n",
       "90152  og7m7k                 what is the best indian movie?   \n",
       "90153  olnost   who's the best mc's, biggie, jay-z, and nas?   \n",
       "\n",
       "                                     Qless  \\\n",
       "0                   people eat spicy food    \n",
       "1                         donut doughnut     \n",
       "2      help finding song video description   \n",
       "3                              rapedno sex   \n",
       "4         jesus h christ whats middle name   \n",
       "...                                    ...   \n",
       "90149          ¿easy work really difficult   \n",
       "90150                      daddy nsfw mean   \n",
       "90151               quiet people strengths   \n",
       "90152                    best indian movie   \n",
       "90153        whos best mcs biggie jayz nas   \n",
       "\n",
       "                                                 Qlemmas  \\\n",
       "0                     ['people', 'eat', 'spicy', 'food']   \n",
       "1                                  ['donut', 'doughnut']   \n",
       "2      ['help', 'finding', 'song', 'video', 'descript...   \n",
       "3                                     ['rapedno', 'sex']   \n",
       "4      ['jesus', 'h', 'christ', 'whats', 'middle', 'n...   \n",
       "...                                                  ...   \n",
       "90149           ['¿easy', 'work', 'really', 'difficult']   \n",
       "90150                          ['daddy', 'nsfw', 'mean']   \n",
       "90151                    ['quiet', 'people', 'strength']   \n",
       "90152                        ['best', 'indian', 'movie']   \n",
       "90153      ['who', 'best', 'mc', 'biggie', 'jayz', 'na']   \n",
       "\n",
       "                                                    Qpos  \\\n",
       "0      [('people', 'NNS'), ('eat', 'VBP'), ('spicy', ...   \n",
       "1                  [('donut', 'NN'), ('doughnut', 'NN')]   \n",
       "2      [('help', 'NN'), ('finding', 'NN'), ('song', '...   \n",
       "3                     [('rapedno', 'NN'), ('sex', 'NN')]   \n",
       "4      [('jesus', 'NN'), ('h', 'NN'), ('christ', 'NN'...   \n",
       "...                                                  ...   \n",
       "90149  [('¿easy', 'JJ'), ('work', 'NN'), ('really', '...   \n",
       "90150  [('daddy', 'JJ'), ('nsfw', 'NNS'), ('mean', 'V...   \n",
       "90151  [('quiet', 'JJ'), ('people', 'NNS'), ('strengt...   \n",
       "90152  [('best', 'JJS'), ('indian', 'JJ'), ('movie', ...   \n",
       "90153  [('who', 'WP'), ('best', 'JJS'), ('mc', 'NN'),...   \n",
       "\n",
       "                                                   Qkeys  \\\n",
       "0                   people_NNS eat_VBP spicy_NN food_NN    \n",
       "1                                  donut_NN doughnut_NN    \n",
       "2      help_NN finding_NN song_NN video_NN descriptio...   \n",
       "3                                     rapedno_NN sex_NN    \n",
       "4      jesus_NN h_NN christ_NN whats_NNS middle_VBP n...   \n",
       "...                                                  ...   \n",
       "90149                                 work_NN really_RB    \n",
       "90150                                 nsfw_NNS mean_VBP    \n",
       "90151                            people_NNS strength_NN    \n",
       "90152                                          movie_NN    \n",
       "90153              who_WP mc_NN biggie_NN jayz_NN na_NN    \n",
       "\n",
       "                                             Qclean  \\\n",
       "0                    people that eat spicy food why   \n",
       "1             donut or doughnut  where are you from   \n",
       "2      help finding a song from a video description   \n",
       "3                          i was rapedno we had sex   \n",
       "4              jesus h christ whats his middle name   \n",
       "...                                             ...   \n",
       "90149           ¿what easy work is really difficult   \n",
       "90150                     daddy what does nsfw mean   \n",
       "90151          quiet people what are your strengths   \n",
       "90152                 what is the best indian movie   \n",
       "90153         whos the best mcs biggie jayz and nas   \n",
       "\n",
       "                                                     ANS  \\\n",
       "0      I never liked spicy food, it just adds a pain ...   \n",
       "1                                    donut, southern usa   \n",
       "2                                       /r/tipofmytongue   \n",
       "3                              if its not a yes, its no.   \n",
       "4      umm... Holy. As in Holy Christ. Isn't this com...   \n",
       "...                                                  ...   \n",
       "90149  None by virtue of it being easy in the first p...   \n",
       "90150                                  not safe for work   \n",
       "90151                      Observing, attention, memory,   \n",
       "90152                               Why is this a quote?   \n",
       "90153              whyd you put your question in quotes?   \n",
       "\n",
       "                                                   Aless  \\\n",
       "0      I never liked spicy food adds pain element don...   \n",
       "1                                     donut southern usa   \n",
       "2                                         rtipofmytongue   \n",
       "3                                                   yes    \n",
       "4          umm Holy As Holy Christ Isnt common knowledge   \n",
       "...                                                  ...   \n",
       "90149                       None virtue easy first place   \n",
       "90150                                          safe work   \n",
       "90151                         Observing attention memory   \n",
       "90152                                          Why quote   \n",
       "90153                           whyd put question quotes   \n",
       "\n",
       "                                                 Alemmas  \\\n",
       "0      ['I', 'never', 'liked', 'spicy', 'food', 'add'...   \n",
       "1                           ['donut', 'southern', 'usa']   \n",
       "2                                     ['rtipofmytongue']   \n",
       "3                                                ['yes']   \n",
       "4      ['umm', 'Holy', 'As', 'Holy', 'Christ', 'Isnt'...   \n",
       "...                                                  ...   \n",
       "90149       ['None', 'virtue', 'easy', 'first', 'place']   \n",
       "90150                                   ['safe', 'work']   \n",
       "90151               ['Observing', 'attention', 'memory']   \n",
       "90152                                   ['Why', 'quote']   \n",
       "90153               ['whyd', 'put', 'question', 'quote']   \n",
       "\n",
       "                                                    Apos  \\\n",
       "0      [('I', 'PRP'), ('never', 'RB'), ('liked', 'VBD...   \n",
       "1      [('donut', 'NN'), ('southern', 'JJ'), ('usa', ...   \n",
       "2                             [('rtipofmytongue', 'NN')]   \n",
       "3                                       [('yes', 'NNS')]   \n",
       "4      [('umm', 'JJ'), ('Holy', 'NNP'), ('As', 'IN'),...   \n",
       "...                                                  ...   \n",
       "90149  [('None', 'NN'), ('virtue', 'NN'), ('easy', 'V...   \n",
       "90150                   [('safe', 'JJ'), ('work', 'NN')]   \n",
       "90151  [('Observing', 'VBG'), ('attention', 'NN'), ('...   \n",
       "90152                  [('Why', 'WRB'), ('quote', 'NN')]   \n",
       "90153  [('whyd', 'NN'), ('put', 'VBD'), ('question', ...   \n",
       "\n",
       "                                                   Akeys  \\\n",
       "0      I_PRP never_RB liked_VBD spicy_NN food_NN add_...   \n",
       "1                                       donut_NN usa_NN    \n",
       "2                                     rtipofmytongue_NN    \n",
       "3                                               yes_NNS    \n",
       "4      Holy_NNP As_IN Holy_NNP Christ_NNP Isnt_NNP kn...   \n",
       "...                                                  ...   \n",
       "90149               None_NN virtue_NN easy_VBP place_NN    \n",
       "90150                                           work_NN    \n",
       "90151              Observing_VBG attention_NN memory_NN    \n",
       "90152                                  Why_WRB quote_NN    \n",
       "90153              whyd_NN put_VBD question_NN quote_NN    \n",
       "\n",
       "                                                  Aclean  \\\n",
       "0      I never liked spicy food it just adds a pain e...   \n",
       "1                                     donut southern usa   \n",
       "2                                         rtipofmytongue   \n",
       "3                                if its not a yes its no   \n",
       "4      umm Holy As in Holy Christ Isnt this common kn...   \n",
       "...                                                  ...   \n",
       "90149  None by virtue of it being easy in the first p...   \n",
       "90150                                  not safe for work   \n",
       "90151                         Observing attention memory   \n",
       "90152                                Why is this a quote   \n",
       "90153               whyd you put your question in quotes   \n",
       "\n",
       "                                                 QA-keys  \\\n",
       "0      people_NNS eat_VBP spicy_NN food_NN  I_PRP nev...   \n",
       "1                 donut_NN doughnut_NN  donut_NN usa_NN    \n",
       "2      help_NN finding_NN song_NN video_NN descriptio...   \n",
       "3                            rapedno_NN sex_NN  yes_NNS    \n",
       "4      jesus_NN h_NN christ_NN whats_NNS middle_VBP n...   \n",
       "...                                                  ...   \n",
       "90149  work_NN really_RB  None_NN virtue_NN easy_VBP ...   \n",
       "90150                        nsfw_NNS mean_VBP  work_NN    \n",
       "90151  people_NNS strength_NN  Observing_VBG attentio...   \n",
       "90152                        movie_NN  Why_WRB quote_NN    \n",
       "90153  who_WP mc_NN biggie_NN jayz_NN na_NN  whyd_NN ...   \n",
       "\n",
       "                                               QA-lemmas  \n",
       "0      ['people', 'eat', 'spicy', 'food', 'I', 'never...  \n",
       "1      ['donut', 'doughnut', 'donut', 'southern', 'usa']  \n",
       "2      ['help', 'finding', 'song', 'video', 'descript...  \n",
       "3                              ['rapedno', 'sex', 'yes']  \n",
       "4      ['jesus', 'h', 'christ', 'whats', 'middle', 'n...  \n",
       "...                                                  ...  \n",
       "90149  ['¿easy', 'work', 'really', 'difficult', 'None...  \n",
       "90150          ['daddy', 'nsfw', 'mean', 'safe', 'work']  \n",
       "90151  ['quiet', 'people', 'strength', 'Observing', '...  \n",
       "90152        ['best', 'indian', 'movie', 'Why', 'quote']  \n",
       "90153  ['who', 'best', 'mc', 'biggie', 'jayz', 'na', ...  \n",
       "\n",
       "[90154 rows x 15 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fasttext\n",
    "from patterns import emoji_pattern\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"db/features.csv\", index_col=0)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "podria ser capa de LSA? (a los embedings tmb?   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models = {}\n",
    "gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdidf_model(df, column, range=(2,3), min_df=20, threshold=1e-3, ngram=False):\n",
    "    \n",
    "    docs = df[column].fillna(\"\")\n",
    "\n",
    "    if ngram:\n",
    "        tdidf = TfidfVectorizer(min_df=min_df, ngram_range=range)\n",
    "    else:\n",
    "        tdidf = TfidfVectorizer(min_df=min_df)\n",
    "\n",
    "    #Create, Normalize and Reduce \n",
    "    model = tdidf.fit_transform(docs)\n",
    "    model = normalize(model, axis=1, norm=\"max\")\n",
    "    model = VarianceThreshold(threshold=threshold).fit_transform(model)\n",
    "\n",
    "    return np.array(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qkeys is a previously prunned pair of lemmas_PoS\n",
    "Models[\"Q-keys\"] = np.array(tdidf_model(df, column=\"Qkeys\", min_df=15))\n",
    "\n",
    "#Qless is with stopwords removed\n",
    "Models[\"Q-ngrams\"] = np.array(tdidf_model(df, column=\"Qless\", range=(2,3), min_df=15, ngram = True))\n",
    "\n",
    "#Question+Answer lemma_keys\n",
    "Models[\"QA-keys\"] = np.array(tdidf_model(df, column=\"QA-keys\", min_df=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "https://kavita-ganesan.com/fasttext-vs-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def w2v_doc_embedding(docs_toks):\n",
    "\n",
    "    docs_model = []\n",
    "    \n",
    "    words_model = Word2Vec(docs_toks, size=300, workers=8, seed=0)\n",
    "\n",
    "    for tokens in docs_toks:\n",
    "        zero_vector = np.zeros(words_model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in words_model.wv:\n",
    "                try:\n",
    "                    vectors.append(words_model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            docs_model.append(avg_vec)\n",
    "        else:\n",
    "            docs_model.append(zero_vector)\n",
    "    return np.array(docs_model)\n",
    "\n",
    "Models[\"w2v-Q\"] = w2v_doc_embedding(list(df[\"Qlemmas\"]))\n",
    "Models[\"w2v-QA\"] = w2v_doc_embedding(list(df[\"QA-lemmas\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Embeddings\n",
    "\n",
    "- SBERT , glove-6B-300\n",
    "    * Q, QLess, QA, QAless  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    glove60 = KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=False, no_header=True)\n",
    "    def glove_doc_embedding(docs_toks, words_model):\n",
    "\n",
    "        docs_model = []\n",
    "        \n",
    "\n",
    "        for tokens in docs_toks:\n",
    "            zero_vector = np.zeros(words_model.vector_size)\n",
    "            vectors = []\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    vectors.append(words_model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                docs_model.append(avg_vec)\n",
    "            else:\n",
    "                docs_model.append(zero_vector)\n",
    "        return docs_model\n",
    "        \n",
    "    Models[\"glove-Q\"] = glove_doc_embedding(list(df[\"Qtoks\"]), glove60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model = SentenceTransformer('nreimers/MiniLM-L6-H384-uncased') #\n",
    "    Models[\"SBERT\"] = model.encode(list(df[\"Q\"].astype('str')), batch_size=70, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "https://github.com/ddangelov/Top2Vec\n",
    "https://pythonrepo.com/repo/ddangelov-Top2Vec-python-natural-language-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "if gpu:\n",
    "    Models[\"top2vec\"] = Top2Vec(list(df[\"Q\"].astype('str')), embedding_model='universal-sentence-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "\n",
    "Models[\"LDA\"] = lda_model = LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 8,\n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 40,\n",
    "                                   workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo voy a probar con KMeans por cuestion de cantidad de datos y recursos, lo que me permite\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html\n",
    "https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92  \n",
    "https://towardsdatascience.com/k-means-clustering-and-the-gap-statistics-4c5d414acd29  \n",
    "https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html   s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "\n",
    "class FaissKMeans:\n",
    "    def __init__(self, n_clus=10, n_init=10, max_iter=50):\n",
    "        self.n_clusters = n_clus\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "        self.kmeans = None\n",
    "        self.cluster_centers_ = None\n",
    "        self.inertia_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.kmeans = faiss.Kmeans(d=X.shape[1],\n",
    "                                   k=self.n_clusters,\n",
    "                                   niter=self.max_iter,\n",
    "                                   nredo=self.n_init)\n",
    "        self.kmeans.train(X.astype(np.float32))\n",
    "        self.cluster_centers_ = self.kmeans.centroids\n",
    "        self.inertia_ = self.kmeans.obj[-1]\n",
    "        #print(self.kmeans.obj)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.kmeans.index.search(X.astype(np.float32), 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "inertia, silhuete, own_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con este dataset el metodo de elbow no se puede utiizar porque el error crece linearmente\n",
    "def get_elbow(X,ran=(8,20)):\n",
    "    #from kneed import KneeLocator, DataGenerator\n",
    "\n",
    "    distortions = []\n",
    "    for i in range(1000,1003):\n",
    "        fkm = FaissKMeans(n_clus=i)\n",
    "        fkm.fit(X, [])\n",
    "        distortions.append(fkm.inertia_)\n",
    "    return distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (<ipython-input-24-54ce6ed39d39>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-54ce6ed39d39>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from sklearn.cluster import KMeans,\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters=30\n",
    "kmc = KMeans(n_clusters=n_clusters,  random_state=0)\n",
    "kmc.fit(X=Models[\"top2vec\"])\n",
    "\n",
    "df[\"label1\"] = kmc.labels_\n",
    "\n",
    "#vemos los primeros clusters, al ser con ngrams  la forma en la que comienza la pregunta tiene mucho peso\n",
    "def print_kmc_clus(n=10):\n",
    "    import random\n",
    "    q_clus = [[] for i in range(n_clusters)]\n",
    "\n",
    "    for sentence_id, cluster_id in enumerate(kmc.labels_):\n",
    "        q_clus[cluster_id].append(df.iloc[sentence_id].Q)\n",
    "\n",
    "    for i, cluster in enumerate(q_clus):\n",
    "        print(f\"Cluster {i+1}, len: {len(cluster)}\" )\n",
    "        for tw in random.sample(cluster, n):\n",
    "            print(tw)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "print_kmc_clus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35882828282828283, 0.7212772127721278)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Compara la cantidad de aciertos con los df de test\n",
    "def check_results():\n",
    "    test_df = pd.read_csv(\"db/test_db.csv\", index_col=0)\n",
    "    true_df = test_df[test_df[\"2\"]]\n",
    "    false_df = test_df[test_df[\"2\"] == False]\n",
    "\n",
    "    n = len(true_df)\n",
    "    m = len(false_df)\n",
    "    same_c = 0\n",
    "    for i in range(n):\n",
    "        if int(df.loc[true_df.iloc[i][0]].label1) ==  int(df.loc[true_df.iloc[i][1]].label1):\n",
    "            same_c += 1\n",
    "    \n",
    "    diff_c = 0\n",
    "    for i in range(m):\n",
    "        if int(df.loc[false_df.iloc[i][0]].label1) !=  int(df.loc[false_df.iloc[i][1]].label1):\n",
    "            diff_c += 1\n",
    "        \n",
    "    return same_c/n, diff_c/m\n",
    "        \n",
    "check_results()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
