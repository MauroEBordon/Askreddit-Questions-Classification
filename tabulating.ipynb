{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resultados(Nclusters, model_type, model, cl_method, tipo de distancia, overall same_c/n, overall diff_c/m, test same_c/n, test diff_c/m, handmade)\n",
    "\n",
    "crear viualizaciones sobre esta tabla\n",
    "puedo crear un sub-testdb de solo las preguntas que voy a tener en el conjunto de test\n",
    "\n",
    "Â¿que se medira?\n",
    "\n",
    "overall score\n",
    "test score\n",
    "handmade test score\n",
    "\n",
    "\n",
    "separo en 2\n",
    "\n",
    "mediciones(m_id, Nclusters, model_type, model, cl_method, tipo de distancia)\n",
    "\n",
    "resultados(m_id, overall same_c/n, overall diff_c/m, test same_c/n, test diff_c/m, handmade)\n",
    "\n",
    "10 20 30 40\n",
    "tdidf, w2v, pretrained embbs\n",
    "[Qkeys, Qclean, Q-ngrams, QA-keys, Qstp], [Qkeys, Qclean, QA-keys], [SBERT-Q, glove-Q]\n",
    "kmeans\n",
    "s2_distance (faiss default), cosine_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fasttext\n",
    "from patterns import emoji_pattern\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"db/features.csv\", index_col=0, converters={'Qtoks': pd.eval,'Qstp': pd.eval,'Qkeys': pd.eval}).dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(df, gpu=False):\n",
    "    Models = {}\n",
    "\n",
    "    tdidf = {}\n",
    "\n",
    "    tdidf[\"Qkeys\"] = tdidf_model(df, column=\"Q-kstr\", min_df=15)\n",
    "    tdidf[\"Qclean\"] = tdidf_model(df, column=\"Qclean\", min_df=15)\n",
    "    tdidf[\"Q-ngrams\"] = tdidf_model(df, column=\"Qclean\", range=(2,3), min_df=15, ngram = True)\n",
    "    tdidf[\"QA-keys\"] = tdidf_model(df, column=\"QA-kstr\", min_df=15)\n",
    "    tdidf[\"Qstp\"] = tdidf_model(df, column=\"Q-stpstr\", min_df=1)\n",
    "\n",
    "    Models[\"tdidf\"] = tdidf\n",
    "\n",
    "    w2v = {}\n",
    "\n",
    "    w2v[\"Qclean\"] = w2v_doc_embedding(list(df[\"Qclean\"].astype(\"str\")))\n",
    "    w2v[\"Qkeys\"] = w2v_doc_embedding(list(df[\"Qkeys\"]))\n",
    "    w2v[\"QA-keys\"] = w2v_doc_embedding(list(df[\"QA-keys\"]))\n",
    "\n",
    "    Models[\"w2v\"] = w2v\n",
    "\n",
    "    if gpu:\n",
    "        pretrained = {}\n",
    "\n",
    "        glove60 = KeyedVectors.load_word2vec_format(\"glove.6B.300d.txt\", binary=False, no_header=True)\n",
    "        sbert_model = SentenceTransformer('nreimers/MiniLM-L6-H384-uncased') #\n",
    "        \n",
    "        pretrained[\"glove-Q\"] = glove_doc_embedding(list(df[\"Qtoks\"]), glove60)\n",
    "        pretrained[\"SBERT-Q\"] = sbert_model.encode(list(df[\"Q\"].astype('str')), batch_size=70, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "        Models[\"pretrain\"] = pretrained\n",
    "\n",
    "    return Models\n",
    "\n",
    "def tdidf_model(df, column, range=(2,3), min_df=20, threshold=1e-3, ngram=False):\n",
    "    \n",
    "    docs = df[column].fillna(\"\")\n",
    "\n",
    "    if ngram:\n",
    "        tdidf = TfidfVectorizer(min_df=min_df, ngram_range=range)\n",
    "    else:\n",
    "        tdidf = TfidfVectorizer(min_df=min_df)\n",
    "\n",
    "    #Create, Normalize and Reduce \n",
    "    model = tdidf.fit_transform(docs)\n",
    "    model = normalize(model, axis=1, norm=\"max\")\n",
    "    model = VarianceThreshold(threshold=threshold).fit_transform(model)\n",
    "\n",
    "    return model.toarray()\n",
    "\n",
    "\n",
    "def w2v_doc_embedding(docs_toks):\n",
    "\n",
    "    docs_model = []\n",
    "    \n",
    "    words_model = Word2Vec(docs_toks, size=300, workers=8, seed=0)\n",
    "\n",
    "    for tokens in docs_toks:\n",
    "        zero_vector = np.zeros(words_model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in words_model.wv:\n",
    "                try:\n",
    "                    vectors.append(words_model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            docs_model.append(avg_vec)\n",
    "        else:\n",
    "            docs_model.append(zero_vector)\n",
    "    return np.array(docs_model)\n",
    "\n",
    "def glove_doc_embedding(docs_toks, words_model):\n",
    "\n",
    "        docs_model = []\n",
    "        \n",
    "\n",
    "        for tokens in docs_toks:\n",
    "            zero_vector = np.zeros(words_model.vector_size)\n",
    "            vectors = []\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    vectors.append(words_model[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if vectors:\n",
    "                vectors = np.asarray(vectors)\n",
    "                avg_vec = vectors.mean(axis=0)\n",
    "                docs_model.append(avg_vec)\n",
    "            else:\n",
    "                docs_model.append(zero_vector)\n",
    "        return docs_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10 20 30 40\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
