{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building the AskReddit Corpus about Covid\n\n**I'm using this 2 datasets from kaggle where they collected a good number of Askreddit Posts.  \nWith a a little processing we can make a hopefully usefull corpus.**\n\n[URL donde correr la notebook](https://www.kaggle.com/mauroebordon/0-askreddit-corpus)","metadata":{}},{"cell_type":"code","source":"!pip install -U pandas\nimport re\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:06:38.380042Z","iopub.execute_input":"2021-11-25T20:06:38.38079Z","iopub.status.idle":"2021-11-25T20:07:09.853625Z","shell.execute_reply.started":"2021-11-25T20:06:38.380738Z","shell.execute_reply":"2021-11-25T20:07:09.852699Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creamos el primer Dataframe con el primer Dataset\n\n#Está separado en 2 archivos (El path es el default de Kaggle, recomiendo correr ahi o tendran q modificar esto)\nqs_df1 = pd.read_csv(\"../input/askreddit-questions-and-answers/reddit_questions.csv\", delimiter=';')\nans_df1 = pd.read_csv(\"../input/askreddit-questions-and-answers/reddit_answers.csv\", delimiter=';')\n\n#Merge y lo limpiamos un poquito\ndf1 = qs_df1.merge(ans_df1, left_on=\"id\", right_on=\"q_id\").drop(columns=[\"datetime\", \"q_id\", \"timestamp\"]).rename(columns={\"text_x\": \"Q\", \"text_y\": \"ANS\", \"votes_x\": \"Qscore\", \"votes_y\": \"ANSscore\"}).drop(columns=[\"Unnamed: 0\"])\n\n#Temporal: solo dejamos los comentarios con más upvotes\ndf1 = df1.groupby(\"id\", as_index=False).agg(lambda x: max(x))\n\ndf1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-25T20:07:09.855224Z","iopub.execute_input":"2021-11-25T20:07:09.855473Z","iopub.status.idle":"2021-11-25T20:07:50.303622Z","shell.execute_reply.started":"2021-11-25T20:07:09.855443Z","shell.execute_reply":"2021-11-25T20:07:50.30304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creamos el segundo dataframe para el otro dataset\n\n#Está separado en 2 archivos\nans_df2 = pd.read_csv(\"../input/a-month-of-askreddit/askreddit_comments.csv\", dtype=str).drop(columns=[ \"permalink\", \"timestamp\", \"author\"])\nqs_df2 = pd.read_csv(\"../input/a-month-of-askreddit/askreddit_posts.csv\", dtype=str).drop(columns=[\"author\", \"timestamp\", \"num_comments\", \"selftext\", \"permalink\"]).rename({\"title\": \"text\"})\n\n#filtramos los comentarios que sean directos al post.\nans_df2 = ans_df2[ans_df2[\"post_id\"] == ans_df2[\"parent_id\"]]\n\n#Normalizamos el id\nans_df2[\"post_id\"] = ans_df2[\"post_id\"].str.replace(\"t3_\", \"\")\n\nans_df2 = ans_df2.drop(columns=[ \"parent_id\"]).rename(columns={\"body\": \"ANS\", \"score\": \"ANSscore\", \"post_id\": \"id\"})\n\n#Ajustamos los nombres para que coincidan\nqs_df2 = qs_df2.rename(columns={\"post_id\": \"id\", \"title\": \"Q\", \"score\": \"Qscore\"})\n\ndf2 = qs_df2.merge(ans_df2, left_on=\"id\", right_on=\"id\")\n\n\ndf2 = df2.groupby(\"id\", as_index=False).first()\n\ndf2","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:13:11.099195Z","iopub.execute_input":"2021-11-25T20:13:11.09955Z","iopub.status.idle":"2021-11-25T20:14:11.260005Z","shell.execute_reply.started":"2021-11-25T20:13:11.099504Z","shell.execute_reply":"2021-11-25T20:14:11.259098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Los concatenamos, Y Ya tenemos un lindo corpus de AskReddit para multiple uso\ndf = pd.concat([df1, df2], ignore_index=True)\n\n#lo guardamos depaso\ndf.to_csv(\"ask-reddit.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:17:41.60143Z","iopub.execute_input":"2021-11-25T20:17:41.60187Z","iopub.status.idle":"2021-11-25T20:17:46.49103Z","shell.execute_reply.started":"2021-11-25T20:17:41.601827Z","shell.execute_reply":"2021-11-25T20:17:46.49017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n#qs1_df[qs1_df[\"text\"].str.contains(r\"deworm\", flags=re.IGNORECASE)]\n\n#qs1_df[qs1_df[\"text\"].str.contains(r\"conspiracy|hoax|tested|pfizer|covid|vax|Sputnik|Moderna|Jab|vacc|corona|virus|Dosis|Cases|Face\\W?Mask|pandemic|quarantine|social distanc\", flags=re.IGNORECASE)]","metadata":{"execution":{"iopub.status.busy":"2021-11-25T20:14:21.632493Z","iopub.execute_input":"2021-11-25T20:14:21.632877Z","iopub.status.idle":"2021-11-25T20:14:26.459093Z","shell.execute_reply.started":"2021-11-25T20:14:21.632838Z","shell.execute_reply":"2021-11-25T20:14:26.458215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}